{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in e:\\project sentiment analysis\\projenv\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in e:\\project sentiment analysis\\projenv\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in e:\\project sentiment analysis\\projenv\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in e:\\project sentiment analysis\\projenv\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in e:\\project sentiment analysis\\projenv\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycrfsuite\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import chain\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def readCorpus(fpath):\n",
    "    sents = []\n",
    "    with open(fpath) as fd:\n",
    "        sent = []\n",
    "        for l in fd:\n",
    "            #lt = l.strip().decode(\"utf8\")\n",
    "            lt = l.strip()\n",
    "            if not lt:\n",
    "                sents.append(sent)\n",
    "                sent = []\n",
    "            else:\n",
    "                w_t = lt.split('\\t')\n",
    "                sent.append([w_t[0], w_t[1]])\n",
    "    return sents\n",
    "\n",
    "def word2features(sent,i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = [\n",
    "        #'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        #'word[-3:]=' + word[-3:],\n",
    "        #'word[-2:]=' + word[-2:],\n",
    "        #'word.isupper=%s' % word.isupper(),\n",
    "        #'word.istitle=%s' % word.istitle(),\n",
    "        #'word.isdigit=%s' % word.isdigit(),\n",
    "        'postag=' + postag,\n",
    "        #'postag[:2]=' + postag[:2],\n",
    "    ]\n",
    "    if i > 0:\n",
    "        pass\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            #'-1:word.istitle=%s' % word1.istitle(),\n",
    "            #'-1:word.isupper=%s' % word1.isupper(),\n",
    "            '-1:postag=' + postag1,\n",
    "            #'-1:postag[:2]=' + postag1[:2],\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        pass\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            #'+1:word.istitle=%s' % word1.istitle(),\n",
    "            #'+1:word.isupper=%s' % word1.isupper(),\n",
    "            '+1:postag=' + postag1,\n",
    "            #'+1:postag[:2]=' + postag1[:2],\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "                \n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start append train set.\n",
      "append train set done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x266840855d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = readCorpus(\"CrfTrain.TXT\")\n",
    "#print(train_sents)\n",
    "x_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "#print(y_train)\n",
    "\n",
    "print(\"start append train set.\")\n",
    "trainer = pycrfsuite.Trainer(algorithm='l2sgd',verbose=False)\n",
    "for xseq, yseq in zip(x_train, y_train):\n",
    "    trainer.append(xseq, yseq)\n",
    "print(\"append train set done.\")\n",
    "trainer.set_params({\n",
    "    #'c1': 0.01,\n",
    "    'c2': 0.01,\n",
    "    'max_iterations': 100,\n",
    "    'feature.possible_transitions': True\n",
    "    })\n",
    "\n",
    "trainer.train(\"trained_model\")\n",
    "test_object = pycrfsuite.Tagger()\n",
    "test_object.open(\"trained_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same as picture but string chahi ali moto cha but nice quality\n",
      "\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ENG', 'O']\n",
      "Predicted: O O O O O O O O O O B-ENG O\n",
      "Correct:   O O O O O O O O O O B-ENG O\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-ENG       1.00      1.00      1.00       264\n",
      "       I-ENG       1.00      1.00      1.00        84\n",
      "       B-NEG       1.00      1.00      1.00        54\n",
      "       I-NEG       1.00      1.00      1.00         6\n",
      "       B-POS       1.00      1.00      1.00       210\n",
      "       I-POS       1.00      1.00      1.00        45\n",
      "           o       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       665\n",
      "   macro avg       1.00      1.00      1.00       665\n",
      "weighted avg       1.00      1.00      1.00       665\n",
      " samples avg       0.20      0.20      0.20       665\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Project Sentiment analysis\\projenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "e:\\Project Sentiment analysis\\projenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Test on the first sentence of the training data\n",
    "#First load the trained model\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('trained_model')\n",
    "\n",
    "#Read the first sentence\n",
    "#print(train_sents[0])\n",
    "example_sent = train_sents[0]\n",
    "print(' '.join(sent2tokens(example_sent)), end='\\n\\n')\n",
    "print(sent2labels(example_sent))\n",
    "\n",
    "#Predict and test\n",
    "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent))))\n",
    "print(\"Correct:  \", ' '.join(sent2labels(example_sent)))\n",
    "\n",
    "#This will give you the precision, recall, F-score\n",
    "def sentiment_classification_report(y_true, y_pred):\n",
    " \n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )\n",
    "\n",
    "##Evaluation of the labeller\n",
    "y_pred = [tagger.tag(xseq) for xseq in x_train]\n",
    "print(sentiment_classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wow', 'kasto', 'ramro', 'product', 'malai', 'manparyo', ':Positive', 'Sentence']\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"Read txt file containing test comment\"\"\"\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    words=[]  # List to store the words\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()  # Remove leading/trailing whitespace\n",
    "            line_words = line.split()  # Split line into individual words\n",
    "            for word in line_words:\n",
    "                words.append(word)  # Add words to the list\n",
    "\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "file_path = \"E:/Project Sentiment analysis/CRF/xyz.txt\"  # Path to the text file\n",
    "\n",
    "# Read the text file and store words in a list\n",
    "word_list = read_text_file(file_path)\n",
    "print(word_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wow', 'kasto', 'ramro', 'product', 'malai', 'manparyo', ':Positive', 'Sentence']\n",
      "['B-ENG', 'I-ENG', 'B-POS', 'I-POS', 'B-POS', 'I-POS', 'B-POS', 'I-POS']\n",
      "Predicted: B-ENG I-ENG B-POS I-POS B-POS I-POS B-POS I-POS\n",
      "Positive Sentiment Count=6\n",
      "Negative Sentiment Count=0\n"
     ]
    }
   ],
   "source": [
    "#Test on the test data \n",
    "#First load the trained model\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('trained_model')\n",
    "\n",
    "#Predict and test\n",
    "print(word_list)\n",
    "\n",
    "new_list = [[word, ''] for word in word_list]\n",
    "\n",
    "# example=[]\n",
    "# example.append((\"hi\"))\n",
    "# print(example)\n",
    "\n",
    "feature=sent2features(new_list)\n",
    "#print(feature)\n",
    "lab=tagger.tag(feature)\n",
    "print(lab)\n",
    "print(\"Predicted:\", ' '.join(lab))\n",
    "\n",
    "PosCount=0\n",
    "NegCount=0\n",
    "for i in lab:\n",
    "    if(i==\"B-POS\" or i==\"I-POS\"):\n",
    "        PosCount+=1\n",
    "    elif(i==\"B-NEG\" or i==\"I-NEG\"):\n",
    "        NegCount+=1\n",
    "print(f\"Positive Sentiment Count={PosCount}\")\n",
    "print(f\"Negative Sentiment Count={NegCount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-crfsuite in e:\\project sentiment analysis\\projenv\\lib\\site-packages (0.9.9)\n",
      "Requirement already satisfied: scikit-learn in e:\\project sentiment analysis\\projenv\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: scipy in e:\\project sentiment analysis\\projenv\\lib\\site-packages (1.11.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in e:\\project sentiment analysis\\projenv\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in e:\\project sentiment analysis\\projenv\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in e:\\project sentiment analysis\\projenv\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install python-crfsuite scikit-learn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn_crfsuite in e:\\project sentiment analysis\\projenv\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in e:\\project sentiment analysis\\projenv\\lib\\site-packages (from sklearn_crfsuite) (0.9.9)\n",
      "Requirement already satisfied: six in e:\\project sentiment analysis\\projenv\\lib\\site-packages (from sklearn_crfsuite) (1.16.0)\n",
      "Requirement already satisfied: tabulate in e:\\project sentiment analysis\\projenv\\lib\\site-packages (from sklearn_crfsuite) (0.9.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in e:\\project sentiment analysis\\projenv\\lib\\site-packages (from sklearn_crfsuite) (4.65.0)\n",
      "Requirement already satisfied: colorama in e:\\project sentiment analysis\\projenv\\lib\\site-packages (from tqdm>=2.0->sklearn_crfsuite) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: scikit-learn\n",
      "Version: 1.3.0\n",
      "Summary: A set of python modules for machine learning and data mining\n",
      "Home-page: http://scikit-learn.org\n",
      "Author: \n",
      "Author-email: \n",
      "License: new BSD\n",
      "Location: e:\\Project Sentiment analysis\\projenv\\Lib\\site-packages\n",
      "Requires: joblib, numpy, scipy, threadpoolctl\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39m# Get the best estimator and its parameters\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m best_trainer \u001b[39m=\u001b[39m random_search\u001b[39m.\u001b[39;49mbest_estimator_\n\u001b[0;32m     42\u001b[0m best_params \u001b[39m=\u001b[39m random_search\u001b[39m.\u001b[39mbest_params_\n\u001b[0;32m     43\u001b[0m \u001b[39mprint\u001b[39m(best_trainer)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "import pycrfsuite\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import chain\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "\n",
    "# Define the parameter grid for hyperparameter optimization\n",
    "param_grid = {\n",
    "    #'c1': uniform(loc=0.01, scale=0.09),  # Uniform distribution between 0.01 and 0.1\n",
    "    'c2': uniform(loc=0.01, scale=0.09),\n",
    "    'max_iterations': [100, 200, 300]  # Test different values\n",
    "    #'feature.possible_transitions': [True, False]  # Test different values\n",
    "}\n",
    "\n",
    "# Create the CRF trainer object\n",
    "# trainer = CRF(algorithm='l2sgd', verbose=False)\n",
    "trainer = CRF(algorithm='l2sgd')\n",
    "\n",
    "# # Append training data\n",
    "# for xseq, yseq in zip(x_train, y_train):\n",
    "#     trainer.append(xseq, yseq)\n",
    "\n",
    "# Create the RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(estimator=trainer, param_distributions=param_grid, n_iter=10, cv=3)\n",
    "\n",
    "train_sents = readCorpus(\"CrfTrain.TXT\")\n",
    "x_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "# Perform hyperparameter optimization\n",
    "try:\n",
    "    random_search.fit(x_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Get the best estimator and its parameters\n",
    "best_trainer = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "print(best_trainer)\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.cross_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m make_scorer\n\u001b[1;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcross_validation\u001b[39;00m \u001b[39mimport\u001b[39;00m cross_val_score\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgrid_search\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomizedSearchCV\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msklearn_crfsuite\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.cross_validation'"
     ]
    }
   ],
   "source": [
    "# example3 = [['kasto',''], ['naramro',''], ['product',''], ['yo',''], \n",
    "#             ['i',''], ['dont',''], ['recommend',''], ['it','']]\n",
    "# y_pred2 = crf.predict_single(sent2features(example3))\n",
    "# print(y_pred2)\n",
    "# y_train = sent2labels(example3)\n",
    "# print(y_train)\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import nltk\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "# define fixed parameters and parameters to search\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(x_train, y_train)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
